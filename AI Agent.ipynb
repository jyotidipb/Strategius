{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e2a67fcb-3c82-498a-927f-fae0ff889640",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the directory to sys.path\n",
    "import sys\n",
    "sys.path.append('/scratch/project_2010376')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "091a5c3d-f799-4916-8425-67f1c70282df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install langchain langchain-openai openai pandas numpy python-dotenv fastapi uvicorn pydantic scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9c8c3667-7266-46d3-a54f-4328d191b16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2c9cee80-5840-4872-a330-b72b73633db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#!pip install langchain langchain-openai openai pandas numpy python-dotenv ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bd6f35bf-5c45-467f-8720-d94a8b6073ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ OpenAI API key found in environment\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import io\n",
    "import json\n",
    "import re\n",
    "import logging\n",
    "import uuid\n",
    "import pickle\n",
    "from typing import Dict, Any, List, Optional\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, \n",
    "                   format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Set your OpenAI API key here if not in .env file\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"your-api-key-here\"\n",
    "\n",
    "# Check for API key\n",
    "OPENAI_API_KEY = \"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\"\n",
    "if not OPENAI_API_KEY:\n",
    "    # Create a widget to input API key\n",
    "    api_key_widget = widgets.Password(\n",
    "        description='OpenAI API Key:',\n",
    "        placeholder='Enter your OpenAI API key',\n",
    "        style={'description_width': 'initial'},\n",
    "        layout=widgets.Layout(width='50%')\n",
    "    )\n",
    "    \n",
    "    def save_api_key(sender):\n",
    "        os.environ[\"OPENAI_API_KEY\"] = sender.value\n",
    "        global OPENAI_API_KEY\n",
    "        OPENAI_API_KEY = sender.value\n",
    "        print(\"API key saved!\")\n",
    "    \n",
    "    api_key_widget.observe(save_api_key, names='value')\n",
    "    display(api_key_widget)\n",
    "else:\n",
    "    print(\"✅ OpenAI API key found in environment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fa7db055-b2bb-4c13-a5ab-68d81657c4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JSONHandler:\n",
    "    @staticmethod\n",
    "    def extract_json(text: str) -> Optional[Dict[str, Any]]:\n",
    "        \"\"\"Extract JSON from text that might contain markdown or other formatting.\"\"\"\n",
    "        if not text or text.strip() == \"\":\n",
    "            logger.warning(\"Empty response received\")\n",
    "            return None\n",
    "            \n",
    "        # First, try to find JSON within markdown code blocks\n",
    "        json_pattern = r\"```(?:json)?\\s*([\\s\\S]*?)\\s*```\"\n",
    "        match = re.search(json_pattern, text)\n",
    "        if match:\n",
    "            content = match.group(1).strip()\n",
    "        else:\n",
    "            # If no code blocks, try to find anything that looks like JSON\n",
    "            # First try to find content between curly braces including the braces\n",
    "            curly_pattern = r\"\\{[\\s\\S]*\\}\"\n",
    "            match = re.search(curly_pattern, text)\n",
    "            if match:\n",
    "                content = match.group(0)\n",
    "            else:\n",
    "                content = text\n",
    "        \n",
    "        # Clean up the content\n",
    "        content = content.strip()\n",
    "        \n",
    "        try:\n",
    "            return json.loads(content)\n",
    "        except json.JSONDecodeError as e:\n",
    "            logger.error(f\"JSON decode error: {e}\")\n",
    "            # Attempt more aggressive cleanup\n",
    "            try:\n",
    "                # Replace any non-JSON syntax that might be causing issues\n",
    "                cleaned = re.sub(r'(\\w+):', r'\"\\1\":', content)  # Convert unquoted keys\n",
    "                cleaned = re.sub(r'\\'', r'\"', cleaned)  # Replace single quotes with double quotes\n",
    "                return json.loads(cleaned)\n",
    "            except json.JSONDecodeError:\n",
    "                logger.error(f\"Failed to parse JSON even after cleanup\")\n",
    "                return None\n",
    "    \n",
    "    @staticmethod\n",
    "    def ensure_valid_json(data: Dict[str, Any], default_schema: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"Ensure the parsed data matches expected schema, filling gaps with defaults.\"\"\"\n",
    "        result = {}\n",
    "        \n",
    "        # For each field in the default schema\n",
    "        for key, default_value in default_schema.items():\n",
    "            # If the key exists in data, use it; otherwise use the default\n",
    "            if key in data:\n",
    "                result[key] = data[key]\n",
    "            else:\n",
    "                logger.warning(f\"Missing key '{key}' in JSON data, using default\")\n",
    "                result[key] = default_value\n",
    "                \n",
    "        return result\n",
    "\n",
    "class ExcelHandler:\n",
    "    @staticmethod\n",
    "    def load_from_excel(file_content) -> pd.DataFrame:\n",
    "        \"\"\"Load data from Excel file content.\"\"\"\n",
    "        try:\n",
    "            df = pd.read_excel(io.BytesIO(file_content))\n",
    "            logger.info(\"Loaded Excel from file content\")\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading Excel file: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    @staticmethod\n",
    "    def validate_excel_format(df: pd.DataFrame) -> bool:\n",
    "        \"\"\"Validate that the Excel file has the required columns.\"\"\"\n",
    "        required_columns = [\"Question\"]\n",
    "        recommended_columns = [\"Answer\", \"Context\", \"Domain\"]\n",
    "        \n",
    "        # Check for required columns\n",
    "        for col in required_columns:\n",
    "            if col not in df.columns:\n",
    "                logger.error(f\"Required column '{col}' missing from Excel file\")\n",
    "                return False\n",
    "        \n",
    "        # Log warning for recommended columns\n",
    "        for col in recommended_columns:\n",
    "            if col not in df.columns:\n",
    "                logger.warning(f\"Recommended column '{col}' missing from Excel file\")\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    @staticmethod\n",
    "    def extract_qa_pairs(df: pd.DataFrame, default_domain: str = \"General\") -> List[Dict[str, Any]]:\n",
    "        \"\"\"Extract question-answer pairs from the DataFrame.\"\"\"\n",
    "        qa_pairs = []\n",
    "        \n",
    "        for _, row in df.iterrows():\n",
    "            # Extract data\n",
    "            question = row.get(\"Question\", \"\")\n",
    "            \n",
    "            # Skip empty questions\n",
    "            if not question or pd.isna(question):\n",
    "                continue\n",
    "                \n",
    "            # Get other fields with fallbacks\n",
    "            answer = row.get(\"Answer\", \"\") if \"Answer\" in df.columns else \"\"\n",
    "            context = row.get(\"Context\", \"No context provided\") if \"Context\" in df.columns else \"No context provided\"\n",
    "            domain = row.get(\"Domain\", default_domain) if \"Domain\" in df.columns else default_domain\n",
    "            \n",
    "            # Ensure we handle NaN values\n",
    "            if pd.isna(answer): answer = \"\"\n",
    "            if pd.isna(context): context = \"No context provided\"\n",
    "            if pd.isna(domain): domain = default_domain\n",
    "            \n",
    "            qa_pairs.append({\n",
    "                \"question\": question,\n",
    "                \"answer\": answer,\n",
    "                \"context\": context,\n",
    "                \"domain\": domain\n",
    "            })\n",
    "        \n",
    "        logger.info(f\"Extracted {len(qa_pairs)} QA pairs from Excel\")\n",
    "        return qa_pairs\n",
    "\n",
    "class LearningStore:\n",
    "    \"\"\"Store and retrieve feedback and learning data.\"\"\"\n",
    "    \n",
    "    def __init__(self, storage_path=\"./feedback_data.pkl\"):\n",
    "        self.storage_path = storage_path\n",
    "        self.data = self._load_data()\n",
    "    \n",
    "    def _load_data(self) -> Dict[str, Any]:\n",
    "        \"\"\"Load stored data or initialize if not present.\"\"\"\n",
    "        if os.path.exists(self.storage_path):\n",
    "            try:\n",
    "                with open(self.storage_path, 'rb') as f:\n",
    "                    return pickle.load(f)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error loading stored data: {str(e)}\")\n",
    "                return self._initialize_data()\n",
    "        else:\n",
    "            return self._initialize_data()\n",
    "    \n",
    "    def _initialize_data(self) -> Dict[str, Any]:\n",
    "        \"\"\"Initialize empty data structure.\"\"\"\n",
    "        return {\n",
    "            \"feedback\": [],\n",
    "            \"strategies\": [],\n",
    "            \"prompt_improvements\": [],\n",
    "            \"performance_metrics\": {\n",
    "                \"average_rating\": 0,\n",
    "                \"strategies_count\": 0\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def save(self):\n",
    "        \"\"\"Save current data to disk.\"\"\"\n",
    "        try:\n",
    "            with open(self.storage_path, 'wb') as f:\n",
    "                pickle.dump(self.data, f)\n",
    "            logger.info(f\"Saved learning data to {self.storage_path}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error saving learning data: {str(e)}\")\n",
    "    \n",
    "    def add_strategy(self, strategy: Dict[str, Any]):\n",
    "        \"\"\"Add a new strategy to the store.\"\"\"\n",
    "        self.data[\"strategies\"].append(strategy)\n",
    "        self.data[\"performance_metrics\"][\"strategies_count\"] += 1\n",
    "        self.save()\n",
    "    \n",
    "    def add_feedback(self, feedback: Dict[str, Any]):\n",
    "        \"\"\"Add feedback for a strategy.\"\"\"\n",
    "        self.data[\"feedback\"].append(feedback)\n",
    "        \n",
    "        # Update metrics\n",
    "        ratings = [f[\"rating\"] for f in self.data[\"feedback\"]]\n",
    "        self.data[\"performance_metrics\"][\"average_rating\"] = sum(ratings) / len(ratings) if ratings else 0\n",
    "        \n",
    "        self.save()\n",
    "    \n",
    "    def get_prompt_improvements(self) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Get prompt improvements based on feedback.\"\"\"\n",
    "        return self.data[\"prompt_improvements\"]\n",
    "    \n",
    "    def add_prompt_improvement(self, improvement: str):\n",
    "        \"\"\"Add a prompt improvement.\"\"\"\n",
    "        self.data[\"prompt_improvements\"].append({\n",
    "            \"improvement\": improvement,\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        })\n",
    "        self.save()\n",
    "    \n",
    "    def get_similar_strategies(self, domain: str, limit: int = 3) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Get similar strategies based on domain.\"\"\"\n",
    "        # Simple implementation just filtered by domain\n",
    "        similar = [s for s in self.data[\"strategies\"] if s[\"domain\"] == domain]\n",
    "        # Sort by creation date, newest first\n",
    "        similar.sort(key=lambda x: x[\"created_at\"], reverse=True)\n",
    "        return similar[:limit]\n",
    "    \n",
    "    def get_performance_metrics(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get current performance metrics.\"\"\"\n",
    "        return self.data[\"performance_metrics\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fbf7d30a-0785-476a-8256-79fdb4ad73d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuestionAnalysisAgent:\n",
    "    def __init__(self, model_name=\"gpt-4o-mini\"):\n",
    "        \"\"\"Initialize the question analysis agent with the specified model.\"\"\"\n",
    "        self.llm = ChatOpenAI(\n",
    "            api_key=OPENAI_API_KEY,\n",
    "            temperature=0.2,\n",
    "            model=model_name\n",
    "        )\n",
    "        \n",
    "        self.prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "        You are an expert at analyzing and improving strategic business questions.\n",
    "        \n",
    "        Question: {question}\n",
    "        Context: {context}\n",
    "        Domain: {domain}\n",
    "        \n",
    "        {prompt_improvements}\n",
    "        \n",
    "        Analyze this question and return JSON with the following structure:\n",
    "        {{\n",
    "          \"quality_score\": float between 0-1,\n",
    "          \"missing_information\": [\"list of missing critical information\"],\n",
    "          \"refined_question\": \"an improved version of the question\",\n",
    "          \"follow_up_questions\": [\"2-3 follow-up questions to gain more insight\"]\n",
    "        }}\n",
    "        \"\"\")\n",
    "\n",
    "        \n",
    "        \n",
    "        # Create a runnable sequence\n",
    "        self.chain = self.prompt | self.llm\n",
    "        \n",
    "        # Default schema for validation\n",
    "        self.default_schema = {\n",
    "            \"quality_score\": 0.5,\n",
    "            \"missing_information\": [\"Question analysis failed\"],\n",
    "            \"refined_question\": \"\",\n",
    "            \"follow_up_questions\": [\"Could you provide more context?\"]\n",
    "        }\n",
    "    \n",
    "    def analyze(self, question: str, context: str = \"No context provided\", \n",
    "                domain: str = \"General\", prompt_improvements: List[str] = None) -> Dict[str, Any]:\n",
    "        \"\"\"Analyze a business question and provide enhancements.\"\"\"\n",
    "        try:\n",
    "            # Log the input\n",
    "            logger.info(f\"Analyzing question in {domain} domain: {question}\")\n",
    "            \n",
    "            # Format prompt improvements\n",
    "            formatted_improvements = \"\"\n",
    "            if prompt_improvements:\n",
    "                formatted_improvements = \"Apply these specific improvements to your analysis:\\n\"\n",
    "                formatted_improvements += \"\\n\".join([f\"- {impr}\" for impr in prompt_improvements])\n",
    "            \n",
    "            # Run the chain\n",
    "            result = self.chain.invoke({\n",
    "                \"question\": question,\n",
    "                \"context\": context,\n",
    "                \"domain\": domain,\n",
    "                \"prompt_improvements\": formatted_improvements\n",
    "            })\n",
    "            \n",
    "            # Extract JSON from response\n",
    "            logger.debug(f\"Raw response: {result.content}\")\n",
    "            json_data = JSONHandler.extract_json(result.content)\n",
    "            \n",
    "            # Validate and fill gaps\n",
    "            if not json_data:\n",
    "                logger.warning(f\"Failed to extract JSON from response for question: {question}\")\n",
    "                # Use defaults with the original question\n",
    "                json_data = self.default_schema.copy()\n",
    "                json_data[\"refined_question\"] = question\n",
    "                return json_data\n",
    "            \n",
    "            # Ensure all expected fields are present\n",
    "            valid_json = JSONHandler.ensure_valid_json(json_data, self.default_schema)\n",
    "            \n",
    "            # If refined question is empty, use the original\n",
    "            if not valid_json[\"refined_question\"]:\n",
    "                valid_json[\"refined_question\"] = question\n",
    "                \n",
    "            return valid_json\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error analyzing question: {str(e)}\")\n",
    "            # Return default values with the original question\n",
    "            result = self.default_schema.copy()\n",
    "            result[\"refined_question\"] = question\n",
    "            return result\n",
    "\n",
    "class AnswerEvaluationAgent:\n",
    "    def __init__(self, model_name=\"gpt-4o-mini\"):\n",
    "        \"\"\"Initialize the answer evaluation agent with the specified model.\"\"\"\n",
    "        self.llm = ChatOpenAI(\n",
    "            api_key=OPENAI_API_KEY,\n",
    "            temperature=0.1,\n",
    "            model=model_name\n",
    "        )\n",
    "        \n",
    "        self.prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "        You are an expert at evaluating business answers for their strategic value.\n",
    "        \n",
    "        Question: {question}\n",
    "        Answer: {answer}\n",
    "        Domain: {domain}\n",
    "        \n",
    "        {prompt_improvements}\n",
    "        \n",
    "        Evaluate this answer and return JSON with the following structure:\n",
    "        {{\n",
    "          \"completeness_score\": float between 0-1,\n",
    "          \"strategic_value_score\": float between 0-1,\n",
    "          \"logical_consistency\": float between 0-1,\n",
    "          \"key_insights\": [\"list of strategic insights from the answer\"],\n",
    "          \"areas_of_concern\": [\"list of potential issues or weaknesses\"]\n",
    "        }}\n",
    "        \"\"\")\n",
    "        \n",
    "        # Create a runnable sequence\n",
    "        self.chain = self.prompt | self.llm\n",
    "        \n",
    "        # Default schema for validation\n",
    "        self.default_schema = {\n",
    "            \"completeness_score\": 0.5,\n",
    "            \"strategic_value_score\": 0.5,\n",
    "            \"logical_consistency\": 0.5,\n",
    "            \"key_insights\": [\"No specific insights identified\"],\n",
    "            \"areas_of_concern\": [\"Unable to properly evaluate the answer\"]\n",
    "        }\n",
    "    \n",
    "    def evaluate(self, question: str, answer: str, domain: str = \"General\", \n",
    "                prompt_improvements: List[str] = None) -> Dict[str, Any]:\n",
    "        \"\"\"Evaluate an answer to a business question.\"\"\"\n",
    "        try:\n",
    "            # Log the input\n",
    "            logger.info(f\"Evaluating answer in {domain} domain\")\n",
    "            logger.debug(f\"Question: {question}\")\n",
    "            logger.debug(f\"Answer: {answer}\")\n",
    "            \n",
    "            # Format prompt improvements\n",
    "            formatted_improvements = \"\"\n",
    "            if prompt_improvements:\n",
    "                formatted_improvements = \"Apply these specific improvements to your evaluation:\\n\"\n",
    "                formatted_improvements += \"\\n\".join([f\"- {impr}\" for impr in prompt_improvements])\n",
    "            \n",
    "            # Run the chain\n",
    "            result = self.chain.invoke({\n",
    "                \"question\": question,\n",
    "                \"answer\": answer,\n",
    "                \"domain\": domain,\n",
    "                \"prompt_improvements\": formatted_improvements\n",
    "            })\n",
    "            \n",
    "            # Extract JSON from response\n",
    "            logger.debug(f\"Raw response: {result.content}\")\n",
    "            json_data = JSONHandler.extract_json(result.content)\n",
    "            \n",
    "            # Validate and fill gaps\n",
    "            if not json_data:\n",
    "                logger.warning(f\"Failed to extract JSON from response for evaluation\")\n",
    "                return self.default_schema\n",
    "            \n",
    "            # Ensure all expected fields are present\n",
    "            valid_json = JSONHandler.ensure_valid_json(json_data, self.default_schema)\n",
    "            \n",
    "            return valid_json\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error evaluating answer: {str(e)}\")\n",
    "            return self.default_schema\n",
    "\n",
    "class StrategyFormulationAgent:\n",
    "    def __init__(self, model_name=\"gpt-4\"):\n",
    "        \"\"\"Initialize the strategy formulation agent with the specified model.\"\"\"\n",
    "        self.llm = ChatOpenAI(\n",
    "            api_key=OPENAI_API_KEY,\n",
    "            temperature=0.5,  # Allow some creativity\n",
    "            model=model_name\n",
    "        )\n",
    "        \n",
    "        self.prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "        You are an expert strategy consultant tasked with formulating comprehensive business strategies.\n",
    "        \n",
    "        Based on the following question-answer pairs and insights:\n",
    "        \n",
    "        Question-Answer Pairs:\n",
    "        {qa_pairs}\n",
    "        \n",
    "        Key Insights:\n",
    "        {key_insights}\n",
    "        \n",
    "        Constraints:\n",
    "        {constraints}\n",
    "        \n",
    "        {similar_strategies}\n",
    "        \n",
    "        {prompt_improvements}\n",
    "        \n",
    "        Formulate a comprehensive business strategy and return it as JSON with the following structure:\n",
    "        {{\n",
    "          \"title\": \"concise strategy title\",\n",
    "          \"description\": \"overview of the strategy (2-3 sentences)\",\n",
    "          \"steps\": [\"step 1\", \"step 2\", \"step 3\", ...],\n",
    "          \"expected_outcomes\": [\"outcome 1\", \"outcome 2\", ...],\n",
    "          \"metrics\": [\"metric 1\", \"metric 2\", ...],\n",
    "          \"confidence_score\": float between 0-1,\n",
    "          \"critical_assumptions\": [\"assumption 1\", \"assumption 2\", ...]\n",
    "        }}\n",
    "        \n",
    "        The strategy should be concrete, actionable, and directly address the issues raised in the questions and answers.\n",
    "        \"\"\")\n",
    "        \n",
    "        # Create a runnable sequence\n",
    "        self.chain = self.prompt | self.llm\n",
    "        \n",
    "        # Default schema for validation\n",
    "        self.default_schema = {\n",
    "            \"title\": \"Default Contingency Strategy\",\n",
    "            \"description\": \"A general approach based on limited information.\",\n",
    "            \"steps\": [\"Gather more information\", \"Conduct a SWOT analysis\", \"Develop targeted initiatives\"],\n",
    "            \"expected_outcomes\": [\"Improved decision-making\", \"Better resource allocation\"],\n",
    "            \"metrics\": [\"Performance against baseline\", \"ROI\"],\n",
    "            \"confidence_score\": 0.3,\n",
    "            \"critical_assumptions\": [\"Limited information availability\"]\n",
    "        }\n",
    "    \n",
    "    def formulate(self, qa_pairs: List[Dict[str, str]], key_insights: List[str], \n",
    "                 constraints: List[str] = None, similar_strategies: List[Dict[str, Any]] = None,\n",
    "                 prompt_improvements: List[str] = None) -> Dict[str, Any]:\n",
    "        \"\"\"Formulate a strategy based on Q&A pairs and insights.\"\"\"\n",
    "        try:\n",
    "            # Format inputs\n",
    "            if not constraints:\n",
    "                constraints = [\"No specific constraints provided\"]\n",
    "                \n",
    "            qa_formatted = \"\\n\\n\".join([f\"Q: {qa['question']}\\nA: {qa['answer']}\" for qa in qa_pairs])\n",
    "            insights_formatted = \"\\n\".join([f\"- {insight}\" for insight in key_insights])\n",
    "            constraints_formatted = \"\\n\".join([f\"- {constraint}\" for constraint in constraints])\n",
    "            \n",
    "            # Format similar strategies if available\n",
    "            similar_strategies_formatted = \"\"\n",
    "            if similar_strategies:\n",
    "                similar_strategies_formatted = \"Here are similar strategies that have been successful in the past:\\n\\n\"\n",
    "                for i, strat in enumerate(similar_strategies, 1):\n",
    "                    similar_strategies_formatted += f\"Strategy {i}: {strat['strategy']['title']}\\n\"\n",
    "                    similar_strategies_formatted += f\"Description: {strat['strategy']['description']}\\n\"\n",
    "                    similar_strategies_formatted += \"Key steps:\\n\"\n",
    "                    for step in strat['strategy']['steps'][:3]:  # Show only first 3 steps\n",
    "                        similar_strategies_formatted += f\"- {step}\\n\"\n",
    "                    similar_strategies_formatted += \"\\n\"\n",
    "            \n",
    "            # Format prompt improvements\n",
    "            prompt_improvements_formatted = \"\"\n",
    "            if prompt_improvements:\n",
    "                prompt_improvements_formatted = \"Apply these specific improvements to your strategy formulation:\\n\"\n",
    "                prompt_improvements_formatted += \"\\n\".join([f\"- {impr}\" for impr in prompt_improvements])\n",
    "            \n",
    "            # Log the input length\n",
    "            logger.info(f\"Formulating strategy based on {len(qa_pairs)} QA pairs and {len(key_insights)} insights\")\n",
    "            \n",
    "            # Run the chain\n",
    "            result = self.chain.invoke({\n",
    "                \"qa_pairs\": qa_formatted,\n",
    "                \"key_insights\": insights_formatted,\n",
    "                \"constraints\": constraints_formatted,\n",
    "                \"similar_strategies\": similar_strategies_formatted,\n",
    "                \"prompt_improvements\": prompt_improvements_formatted\n",
    "            })\n",
    "            \n",
    "            # Extract JSON from response\n",
    "            logger.debug(f\"Raw response: {result.content}\")\n",
    "            json_data = JSONHandler.extract_json(result.content)\n",
    "            \n",
    "            # Validate and fill gaps\n",
    "            if not json_data:\n",
    "                logger.warning(\"Failed to extract JSON from strategy response\")\n",
    "                return self.default_schema\n",
    "            \n",
    "            # Ensure all expected fields are present\n",
    "            valid_json = JSONHandler.ensure_valid_json(json_data, self.default_schema)\n",
    "            \n",
    "            return valid_json\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error formulating strategy: {str(e)}\")\n",
    "            return self.default_schema\n",
    "\n",
    "class ContinuousLearningAgent:\n",
    "    def __init__(self, model_name=\"gpt-4o\"):\n",
    "        \"\"\"Initialize the continuous learning agent with the specified model.\"\"\"\n",
    "        self.llm = ChatOpenAI(\n",
    "            api_key=OPENAI_API_KEY,\n",
    "            temperature=0.3,\n",
    "            model=model_name\n",
    "        )\n",
    "        \n",
    "        self.prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "        You are an AI improvement specialist focused on analyzing feedback and enhancing prompt quality.\n",
    "        \n",
    "        Based on the following feedback history and performance metrics:\n",
    "        \n",
    "        Feedback History:\n",
    "        {feedback_history}\n",
    "        \n",
    "        Performance Metrics:\n",
    "        {performance_metrics}\n",
    "        \n",
    "        Current Prompt Improvements:\n",
    "        {current_improvements}\n",
    "        \n",
    "        Your task is to analyze this data and suggest 1-3 specific, actionable improvements to our prompts or process to enhance strategy generation quality.\n",
    "        \n",
    "        Provide your response in JSON format:\n",
    "        {{\n",
    "          \"analysis\": \"your detailed analysis of the feedback and performance\",\n",
    "          \"improvement_suggestions\": [\"improvement 1\", \"improvement 2\", \"improvement 3\"],\n",
    "          \"expected_impact\": \"description of how these improvements will enhance performance\"\n",
    "        }}\n",
    "        \"\"\")\n",
    "        \n",
    "        # Create a runnable sequence\n",
    "        self.chain = self.prompt | self.llm\n",
    "        \n",
    "        # Default schema for validation\n",
    "        self.default_schema = {\n",
    "            \"analysis\": \"Insufficient data to perform detailed analysis.\",\n",
    "            \"improvement_suggestions\": [\"Collect more diverse feedback\"],\n",
    "            \"expected_impact\": \"Better understanding of performance patterns\"\n",
    "        }\n",
    "    \n",
    "    def analyze_feedback(self, feedback_history: List[Dict[str, Any]], \n",
    "                         performance_metrics: Dict[str, Any],\n",
    "                         current_improvements: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "        \"\"\"Analyze feedback and suggest improvements.\"\"\"\n",
    "        try:\n",
    "            # Format inputs\n",
    "            if not feedback_history:\n",
    "                return self.default_schema\n",
    "                \n",
    "            # Format feedback history\n",
    "            feedback_formatted = \"\"\n",
    "            for i, fb in enumerate(feedback_history[-5:], 1):  # Use last 5 feedbacks\n",
    "                feedback_formatted += f\"Feedback {i}:\\n\"\n",
    "                feedback_formatted += f\"Rating: {fb.get('rating', 'Unknown')}/5\\n\"\n",
    "                if fb.get('comments'):\n",
    "                    feedback_formatted += f\"Comments: {fb.get('comments')}\\n\"\n",
    "                if fb.get('results'):\n",
    "                    feedback_formatted += \"Results:\\n\"\n",
    "                    for k, v in fb.get('results', {}).items():\n",
    "                        feedback_formatted += f\"- {k}: {v}\\n\"\n",
    "                feedback_formatted += \"\\n\"\n",
    "            \n",
    "            # Format metrics\n",
    "            metrics_formatted = \"\\n\".join([f\"{k}: {v}\" for k, v in performance_metrics.items()])\n",
    "            \n",
    "            # Format current improvements\n",
    "            improvements_formatted = \"\\n\".join([f\"- {impr.get('improvement', '')}\" for impr in current_improvements[-5:]])\n",
    "            \n",
    "            # Run the chain\n",
    "            result = self.chain.invoke({\n",
    "                \"feedback_history\": feedback_formatted,\n",
    "                \"performance_metrics\": metrics_formatted,\n",
    "                \"current_improvements\": improvements_formatted or \"No current improvements\"\n",
    "            })\n",
    "            \n",
    "            # Extract JSON from response\n",
    "            logger.debug(f\"Raw response: {result.content}\")\n",
    "            json_data = JSONHandler.extract_json(result.content)\n",
    "            \n",
    "            # Validate and fill gaps\n",
    "            if not json_data:\n",
    "                logger.warning(\"Failed to extract JSON from learning agent response\")\n",
    "                return self.default_schema\n",
    "            \n",
    "            # Ensure all expected fields are present\n",
    "            valid_json = JSONHandler.ensure_valid_json(json_data, self.default_schema)\n",
    "            \n",
    "            return valid_json\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error analyzing feedback: {str(e)}\")\n",
    "            return self.default_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e3846f74-7f92-4bc2-bf3d-0c94748ab1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StrategyEngine:\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the strategy engine with its component agents.\"\"\"\n",
    "        self.question_agent = QuestionAnalysisAgent()\n",
    "        self.answer_agent = AnswerEvaluationAgent()\n",
    "        self.strategy_agent = StrategyFormulationAgent()\n",
    "        self.learning_agent = ContinuousLearningAgent()\n",
    "        \n",
    "        # Initialize learning store\n",
    "        self.learning_store = LearningStore()\n",
    "        \n",
    "        # Get prompt improvements from store\n",
    "        self.prompt_improvements = [item['improvement'] for item in self.learning_store.get_prompt_improvements()]\n",
    "        \n",
    "    def process_qa_pair(self, question: str, context: str, domain: str, answer: str) -> Dict[str, Any]:\n",
    "        \"\"\"Process a question-answer pair through the analysis pipeline.\"\"\"\n",
    "        # Step 1: Analyze the question\n",
    "        question_analysis = self.question_agent.analyze(\n",
    "            question, \n",
    "            context, \n",
    "            domain, \n",
    "            self.prompt_improvements\n",
    "        )\n",
    "        \n",
    "        # Get the refined question\n",
    "        refined_question = question_analysis.get(\"refined_question\", question)\n",
    "        \n",
    "        # Step 2: Evaluate the answer\n",
    "        answer_evaluation = self.answer_agent.evaluate(\n",
    "            refined_question, \n",
    "            answer, \n",
    "            domain,\n",
    "            self.prompt_improvements\n",
    "        )\n",
    "        \n",
    "        logger.info(f\"Processed QA pair in {domain} domain\")\n",
    "        \n",
    "        return {\n",
    "            \"original_question\": question,\n",
    "            \"refined_question\": refined_question,\n",
    "            \"question_analysis\": question_analysis,\n",
    "            \"answer\": answer,\n",
    "            \"answer_evaluation\": answer_evaluation\n",
    "        }\n",
    "    \n",
    "    def generate_strategy(self, qa_pairs: List[Dict[str, Any]], domain: str, \n",
    "                         constraints: List[str] = None) -> Dict[str, Any]:\n",
    "        \"\"\"Generate a strategy based on multiple question-answer pairs.\"\"\"\n",
    "        processed_pairs = []\n",
    "        all_key_insights = []\n",
    "        qa_for_strategy = []\n",
    "        \n",
    "        # Process each QA pair if they haven't been processed\n",
    "        for qa_pair in qa_pairs:\n",
    "            # Check if this is an original question or already processed pair\n",
    "            if \"original_question\" in qa_pair:\n",
    "                # Already processed\n",
    "                processed_pairs.append(qa_pair)\n",
    "                \n",
    "                # Extract key insights\n",
    "                if \"answer_evaluation\" in qa_pair and \"key_insights\" in qa_pair[\"answer_evaluation\"]:\n",
    "                    all_key_insights.extend(qa_pair[\"answer_evaluation\"][\"key_insights\"])\n",
    "                \n",
    "                # Format for strategy generation\n",
    "                qa_for_strategy.append({\n",
    "                    \"question\": qa_pair[\"refined_question\"],\n",
    "                    \"answer\": qa_pair[\"answer\"]\n",
    "                })\n",
    "            else:\n",
    "                # Need to process this pair\n",
    "                question = qa_pair.get(\"question\", \"\")\n",
    "                context = qa_pair.get(\"context\", \"No context provided\")\n",
    "                answer = qa_pair.get(\"answer\", \"\")\n",
    "                domain_specific = qa_pair.get(\"domain\", domain)\n",
    "                \n",
    "                processed = self.process_qa_pair(question, context, domain_specific, answer)\n",
    "                processed_pairs.append(processed)\n",
    "                \n",
    "                # Extract key insights\n",
    "                if \"answer_evaluation\" in processed and \"key_insights\" in processed[\"answer_evaluation\"]:\n",
    "                    all_key_insights.extend(processed[\"answer_evaluation\"][\"key_insights\"])\n",
    "                \n",
    "                # Format for strategy generation\n",
    "                qa_for_strategy.append({\n",
    "                    \"question\": processed[\"refined_question\"],\n",
    "                    \"answer\": processed[\"answer\"]\n",
    "                })\n",
    "        \n",
    "        logger.info(f\"Generating strategy based on {len(processed_pairs)} QA pairs with {len(all_key_insights)} insights\")\n",
    "        \n",
    "        # Get similar strategies from the learning store\n",
    "        similar_strategies = self.learning_store.get_similar_strategies(domain)\n",
    "        \n",
    "        # Formulate the strategy\n",
    "        strategy = self.strategy_agent.formulate(\n",
    "            qa_for_strategy, \n",
    "            all_key_insights, \n",
    "            constraints,\n",
    "            similar_strategies,\n",
    "            self.prompt_improvements\n",
    "        )\n",
    "        \n",
    "        # Add metadata\n",
    "        strategy_id = str(uuid.uuid4())\n",
    "        timestamp = datetime.now().isoformat()\n",
    "        \n",
    "        result = {\n",
    "            \"strategy_id\": strategy_id,\n",
    "            \"created_at\": timestamp,\n",
    "            \"domain\": domain,\n",
    "            \"strategy\": strategy,\n",
    "            \"source_qa_pairs\": processed_pairs,\n",
    "            \"key_insights\": all_key_insights\n",
    "        }\n",
    "        \n",
    "        # Store in learning store\n",
    "        self.learning_store.add_strategy(result)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def process_excel_file(self, file_content, domain: str = \"General\", \n",
    "                          constraints: List[str] = None) -> Dict[str, Any]:\n",
    "        \"\"\"Process an Excel file to generate a strategy.\"\"\"\n",
    "        # Load the Excel file\n",
    "        df = ExcelHandler.load_from_excel(file_content)\n",
    "        \n",
    "        # Validate the format\n",
    "        if not ExcelHandler.validate_excel_format(df):\n",
    "            raise ValueError(\"Excel file format is invalid\")\n",
    "        \n",
    "        # Extract QA pairs\n",
    "        qa_pairs = ExcelHandler.extract_qa_pairs(df, domain)\n",
    "        \n",
    "        if not qa_pairs:\n",
    "            raise ValueError(\"No valid question-answer pairs found in the Excel file\")\n",
    "        \n",
    "        # Generate strategy\n",
    "        result = self.generate_strategy(qa_pairs, domain, constraints)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def record_feedback(self, strategy_id: str, rating: int, comments: str = None, \n",
    "                        results: Dict[str, Any] = None) -> Dict[str, Any]:\n",
    "        \"\"\"Record feedback for a generated strategy and learn from it.\"\"\"\n",
    "        feedback = {\n",
    "            \"feedback_id\": str(uuid.uuid4()),\n",
    "            \"strategy_id\": strategy_id,\n",
    "            \"rating\": rating,\n",
    "            \"comments\": comments,\n",
    "            \"results\": results,\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        # Add to learning store\n",
    "        self.learning_store.add_feedback(feedback)\n",
    "        \n",
    "        # Check if we have enough feedback to learn from\n",
    "        if len(self.learning_store.data[\"feedback\"]) >= 3:\n",
    "            self._learn_from_feedback()\n",
    "        \n",
    "        logger.info(f\"Recorded feedback for strategy {strategy_id} with rating {rating}\")\n",
    "        \n",
    "        return feedback\n",
    "    \n",
    "    def _learn_from_feedback(self):\n",
    "        \"\"\"Learn from feedback and update prompt improvements.\"\"\"\n",
    "        # Get feedback history and performance metrics\n",
    "        feedback_history = self.learning_store.data[\"feedback\"]\n",
    "        performance_metrics = self.learning_store.get_performance_metrics()\n",
    "        current_improvements = self.learning_store.get_prompt_improvements()\n",
    "        \n",
    "        # Analyze feedback\n",
    "        analysis = self.learning_agent.analyze_feedback(\n",
    "            feedback_history,\n",
    "            performance_metrics,\n",
    "            current_improvements\n",
    "        )\n",
    "        \n",
    "        # Add new improvements to the store\n",
    "        for suggestion in analysis.get(\"improvement_suggestions\", []):\n",
    "            self.learning_store.add_prompt_improvement(suggestion)\n",
    "        \n",
    "        # Update prompt improvements\n",
    "        self.prompt_improvements = [item['improvement'] for item in self.learning_store.get_prompt_improvements()]\n",
    "        \n",
    "        logger.info(f\"Updated prompt improvements based on feedback analysis\")\n",
    "    \n",
    "    def get_accuracy_metrics(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get current accuracy and performance metrics.\"\"\"\n",
    "        return self.learning_store.get_performance_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b2b75aaf-45f1-4cea-ac4f-63cc8c31519d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function to format strategy display in HTML\n",
    "def display_strategy_html(result):\n",
    "    \"\"\"Display a strategy in formatted HTML.\"\"\"\n",
    "    if not result:\n",
    "        return\n",
    "        \n",
    "    strategy = result[\"strategy\"]\n",
    "    \n",
    "    # Build HTML content\n",
    "    html_content = f\"\"\"\n",
    "    <div style=\"padding: 20px; background: #f9f9f9; border-radius: 10px; margin: 20px 0;\">\n",
    "        <h2 style=\"color: #2c3e50; border-bottom: 2px solid #3498db; padding-bottom: 10px;\">\n",
    "            {strategy['title']}\n",
    "        </h2>\n",
    "        \n",
    "        <p style=\"font-size: 16px; line-height: 1.6; margin-top: 15px;\">\n",
    "            {strategy['description']}\n",
    "        </p>\n",
    "        \n",
    "        <h3 style=\"color: #2c3e50; margin-top: 20px;\">Implementation Steps</h3>\n",
    "        <ol style=\"margin-left: 20px;\">\n",
    "    \"\"\"\n",
    "    \n",
    "    # Add steps\n",
    "    for step in strategy['steps']:\n",
    "        html_content += f\"<li style='margin: 10px 0;'>{step}</li>\"\n",
    "    \n",
    "    html_content += \"\"\"\n",
    "        </ol>\n",
    "        \n",
    "        <h3 style=\"color: #2c3e50; margin-top: 20px;\">Expected Outcomes</h3>\n",
    "        <ul style=\"margin-left: 20px;\">\n",
    "    \"\"\"\n",
    "    \n",
    "    # Add outcomes\n",
    "    for outcome in strategy['expected_outcomes']:\n",
    "        html_content += f\"<li style='margin: 8px 0;'>{outcome}</li>\"\n",
    "    \n",
    "    html_content += \"\"\"\n",
    "        </ul>\n",
    "        \n",
    "        <h3 style=\"color: #2c3e50; margin-top: 20px;\">Key Performance Metrics</h3>\n",
    "        <ul style=\"margin-left: 20px;\">\n",
    "    \"\"\"\n",
    "    \n",
    "    # Add metrics\n",
    "    for metric in strategy['metrics']:\n",
    "        html_content += f\"<li style='margin: 8px 0;'>{metric}</li>\"\n",
    "    \n",
    "    # Add confidence score\n",
    "    html_content += f\"\"\"\n",
    "        </ul>\n",
    "        \n",
    "        <div style=\"margin-top: 20px; padding: 10px; background: #eaf2f8; border-radius: 5px;\">\n",
    "            <strong>Confidence Score:</strong> {strategy['confidence_score']:.2f}\n",
    "        </div>\n",
    "        \n",
    "        <h3 style=\"color: #2c3e50; margin-top: 20px;\">Critical Assumptions</h3>\n",
    "        <ul style=\"margin-left: 20px;\">\n",
    "    \"\"\"\n",
    "    \n",
    "    # Add assumptions\n",
    "    for assumption in strategy['critical_assumptions']:\n",
    "        html_content += f\"<li style='margin: 8px 0;'>{assumption}</li>\"\n",
    "    \n",
    "    html_content += \"\"\"\n",
    "        </ul>\n",
    "    </div>\n",
    "    \"\"\"\n",
    "    \n",
    "    return HTML(html_content)\n",
    "\n",
    "# Create sample Excel file for testing\n",
    "def create_sample_excel(filename=\"sample_questions.xlsx\"):\n",
    "    \"\"\"Create a sample Excel file with questions and answers.\"\"\"\n",
    "    data = [\n",
    "        {\n",
    "            \"Question\": \"How can we increase customer retention?\",\n",
    "            \"Context\": \"E-commerce business selling consumer electronics\",\n",
    "            \"Domain\": \"Marketing\",\n",
    "            \"Answer\": \"Our data shows that customers who make a second purchase within 30 days of their first purchase have a 70% higher lifetime value. Currently only 15% of new customers make a second purchase within that window.\"\n",
    "        },\n",
    "        {\n",
    "            \"Question\": \"What channels are most effective for customer acquisition?\",\n",
    "            \"Context\": \"E-commerce business selling consumer electronics\",\n",
    "            \"Domain\": \"Marketing\",\n",
    "            \"Answer\": \"Social media ads bring in the most volume (45% of new customers) but at a higher CAC ($50). SEO brings in 25% of new customers at $30 CAC. Referrals bring in 15% at $20 CAC but have been declining in effectiveness.\"\n",
    "        },\n",
    "        {\n",
    "            \"Question\": \"What is our current profit margin by product category?\",\n",
    "            \"Context\": \"E-commerce business selling consumer electronics\",\n",
    "            \"Domain\": \"Finance\",\n",
    "            \"Answer\": \"Smartphones: 15% margin, Accessories: 45% margin, Laptops: 10% margin, Audio equipment: 30% margin. Accessories and audio equipment have shown margin improvements over the last quarter.\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_excel(filename, index=False)\n",
    "    \n",
    "    print(f\"✅ Sample Excel file created: {filename}\")\n",
    "    print(f\"You can now upload this file using the upload widget.\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Create function to display metrics\n",
    "def show_metrics():\n",
    "    \"\"\"Display current performance metrics.\"\"\"\n",
    "    engine = StrategyEngine()\n",
    "    metrics = engine.get_accuracy_metrics()\n",
    "    \n",
    "    # Build HTML content\n",
    "    html_content = \"\"\"\n",
    "    <div style=\"padding: 15px; background: #f5f5f5; border-radius: 8px; margin: 15px 0;\">\n",
    "        <h3 style=\"color: #2c3e50; border-bottom: 1px solid #bdc3c7; padding-bottom: 8px;\">\n",
    "            Performance Metrics\n",
    "        </h3>\n",
    "        <table style=\"width: 100%; border-collapse: collapse; margin-top: 10px;\">\n",
    "            <tr style=\"background: #eaecee;\">\n",
    "                <th style=\"padding: 8px; text-align: left; border: 1px solid #bdc3c7;\">Metric</th>\n",
    "                <th style=\"padding: 8px; text-align: left; border: 1px solid #bdc3c7;\">Value</th>\n",
    "            </tr>\n",
    "    \"\"\"\n",
    "    \n",
    "    # Add metrics rows\n",
    "    row_bg = [\"#ffffff\", \"#f9f9f9\"]  # Alternating row background colors\n",
    "    i = 0\n",
    "    for key, value in metrics.items():\n",
    "        html_content += f\"\"\"\n",
    "            <tr style=\"background: {row_bg[i % 2]};\">\n",
    "                <td style=\"padding: 8px; border: 1px solid #bdc3c7;\">{key}</td>\n",
    "                <td style=\"padding: 8px; border: 1px solid #bdc3c7;\">{value}</td>\n",
    "            </tr>\n",
    "        \"\"\"\n",
    "        i += 1\n",
    "    \n",
    "    html_content += \"\"\"\n",
    "        </table>\n",
    "    </div>\n",
    "    \"\"\"\n",
    "    \n",
    "    return HTML(html_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9254b401-50b5-431d-862d-326e55b7523e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1a0201f6a37491d95b85426dc72b648",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FileUpload(value=(), accept='.xlsx, .xls', description='Upload Excel')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aece8dc2f8784c0586d8ce079d2c6afa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Text(value='Marketing', description='Domain:', placeholder='Enter business domain', style=TextS…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e23d68e7288742cc83aa1fa41f331085",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(button_style='success', description='Generate Strategy', icon='check', style=ButtonStyle(), tooltip='Cl…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b6fe7a2b6fd4b16a665717b407d72c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4d83e8cb7d14e4a874677f5719c05b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<h3>Provide Feedback</h3>')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fc7913b914a4c2aa28149c4310808d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(IntSlider(value=4, continuous_update=False, description='Rating:', max=5, min=1, style=SliderSt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a554d787991545b084f66e1828fc6df2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4739c23caee460c9cbefe3fc016f974",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<h3>Performance Metrics</h3>')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b33732db5111439cb578fd8f4bec26d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Show Metrics', icon='chart-bar', style=ButtonStyle(), tooltip='Click to show current perfo…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc2798efac8e40749e5b064ba5c28c8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb6b3b595e2b41b1b15ad0cebeb58a50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<h3>Create Sample Data</h3>')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f024b5a30514771bc11f2a842868f80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Create Sample Excel', icon='file-excel', style=ButtonStyle(), tooltip='Click to create a s…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9be649391d2b404cbb1eadb92106587e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# File Upload Widget\n",
    "upload_widget = widgets.FileUpload(\n",
    "    description='Upload Excel',\n",
    "    accept='.xlsx, .xls',\n",
    "    multiple=False\n",
    ")\n",
    "display(upload_widget)\n",
    "\n",
    "# Domain and Constraints Widgets\n",
    "domain_widget = widgets.Text(\n",
    "    value='Marketing',\n",
    "    description='Domain:',\n",
    "    placeholder='Enter business domain',\n",
    "    disabled=False,\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "constraints_widget = widgets.Textarea(\n",
    "    value='Budget is limited to $50,000\\nMust be implemented within 90 days',\n",
    "    description='Constraints:',\n",
    "    placeholder='Enter constraints (one per line)',\n",
    "    disabled=False,\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "display(widgets.VBox([domain_widget, constraints_widget]))\n",
    "\n",
    "# Variables to store the current strategy\n",
    "current_strategy = None\n",
    "strategy_output = widgets.Output()\n",
    "\n",
    "# Function to process the uploaded file\n",
    "def process_uploaded_excel(b):\n",
    "    \"\"\"Process the uploaded Excel file and generate a strategy.\"\"\"\n",
    "    global current_strategy\n",
    "    \n",
    "    with strategy_output:\n",
    "        strategy_output.clear_output()\n",
    "        \n",
    "        if not upload_widget.value:\n",
    "            print(\"⚠️ Please upload an Excel file first.\")\n",
    "            return\n",
    "        \n",
    "        try:\n",
    "            # Handle different possible structures of upload_widget.value\n",
    "            file_content = None\n",
    "            \n",
    "            # Check if it's a tuple (newer ipywidgets versions may return this)\n",
    "            if isinstance(upload_widget.value, tuple):\n",
    "                # If it's a tuple with content\n",
    "                if len(upload_widget.value) > 0 and hasattr(upload_widget.value[0], 'content'):\n",
    "                    file_content = upload_widget.value[0].content\n",
    "                else:\n",
    "                    # Try extracting content directly\n",
    "                    file_content = upload_widget.value\n",
    "            \n",
    "            # Check if it's a dictionary (older ipywidgets versions)\n",
    "            elif isinstance(upload_widget.value, dict):\n",
    "                # Get the first file\n",
    "                file_info = next(iter(upload_widget.value.values()))\n",
    "                file_content = file_info['content']\n",
    "            \n",
    "            # If we couldn't extract content\n",
    "            if file_content is None:\n",
    "                print(\"❌ Could not extract file content from the upload widget.\")\n",
    "                print(f\"Upload widget value type: {type(upload_widget.value)}\")\n",
    "                return\n",
    "            \n",
    "            # Get domain and constraints\n",
    "            domain = domain_widget.value or \"General\"\n",
    "            constraints = [c.strip() for c in constraints_widget.value.split('\\n') if c.strip()]\n",
    "            \n",
    "            # Initialize the engine\n",
    "            engine = StrategyEngine()\n",
    "            \n",
    "            # Process the file\n",
    "            print(f\"🔍 Processing Excel file for {domain} domain...\")\n",
    "            result = engine.process_excel_file(file_content, domain, constraints)\n",
    "            \n",
    "            # Store the result\n",
    "            current_strategy = result\n",
    "            \n",
    "            # Display the result\n",
    "            display(display_strategy_html(result))\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error: {str(e)}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "\n",
    "# Create process button\n",
    "process_button = widgets.Button(\n",
    "    description='Generate Strategy',\n",
    "    disabled=False,\n",
    "    button_style='success',\n",
    "    tooltip='Click to process the Excel file and generate a strategy',\n",
    "    icon='check'\n",
    ")\n",
    "\n",
    "process_button.on_click(process_uploaded_excel)\n",
    "display(process_button)\n",
    "display(strategy_output)\n",
    "\n",
    "# Feedback widgets\n",
    "feedback_output = widgets.Output()\n",
    "\n",
    "rating_widget = widgets.IntSlider(\n",
    "    value=4,\n",
    "    min=1,\n",
    "    max=5,\n",
    "    step=1,\n",
    "    description='Rating:',\n",
    "    disabled=False,\n",
    "    continuous_update=False,\n",
    "    orientation='horizontal',\n",
    "    readout=True,\n",
    "    readout_format='d',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "comments_widget = widgets.Textarea(\n",
    "    value='',\n",
    "    placeholder='Enter your feedback or comments on the strategy',\n",
    "    description='Comments:',\n",
    "    disabled=False,\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "# Function to submit feedback\n",
    "def submit_feedback(b):\n",
    "    global current_strategy\n",
    "    \n",
    "    with feedback_output:\n",
    "        feedback_output.clear_output()\n",
    "        \n",
    "        if not current_strategy:\n",
    "            print(\"⚠️ Please generate a strategy first.\")\n",
    "            return\n",
    "        \n",
    "        # Initialize the engine\n",
    "        engine = StrategyEngine()\n",
    "        \n",
    "        # Record feedback\n",
    "        try:\n",
    "            feedback = engine.record_feedback(\n",
    "                strategy_id=current_strategy[\"strategy_id\"],\n",
    "                rating=rating_widget.value,\n",
    "                comments=comments_widget.value\n",
    "            )\n",
    "            \n",
    "            print(\"✅ Thank you for your feedback! This will help improve future strategies.\")\n",
    "            print(f\"📝 Feedback ID: {feedback['feedback_id']}\")\n",
    "            \n",
    "            # Reset feedback form\n",
    "            comments_widget.value = ''\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error recording feedback: {str(e)}\")\n",
    "\n",
    "# Create feedback button\n",
    "feedback_button = widgets.Button(\n",
    "    description='Submit Feedback',\n",
    "    disabled=False,\n",
    "    button_style='info',\n",
    "    tooltip='Click to submit feedback on the strategy',\n",
    "    icon='comment'\n",
    ")\n",
    "\n",
    "feedback_button.on_click(submit_feedback)\n",
    "\n",
    "display(widgets.HTML(\"<h3>Provide Feedback</h3>\"))\n",
    "display(widgets.VBox([rating_widget, comments_widget, feedback_button]))\n",
    "display(feedback_output)\n",
    "\n",
    "# Metrics button\n",
    "metrics_output = widgets.Output()\n",
    "\n",
    "def display_metrics(b):\n",
    "    with metrics_output:\n",
    "        metrics_output.clear_output()\n",
    "        display(show_metrics())\n",
    "\n",
    "metrics_button = widgets.Button(\n",
    "    description='Show Metrics',\n",
    "    disabled=False,\n",
    "    button_style='',\n",
    "    tooltip='Click to show current performance metrics',\n",
    "    icon='chart-bar'\n",
    ")\n",
    "\n",
    "metrics_button.on_click(display_metrics)\n",
    "\n",
    "display(widgets.HTML(\"<h3>Performance Metrics</h3>\"))\n",
    "display(metrics_button)\n",
    "display(metrics_output)\n",
    "\n",
    "# Create sample Excel button\n",
    "sample_output = widgets.Output()\n",
    "\n",
    "def create_sample(b):\n",
    "    with sample_output:\n",
    "        sample_output.clear_output()\n",
    "        create_sample_excel()\n",
    "\n",
    "sample_button = widgets.Button(\n",
    "    description='Create Sample Excel',\n",
    "    disabled=False,\n",
    "    button_style='',\n",
    "    tooltip='Click to create a sample Excel file',\n",
    "    icon='file-excel'\n",
    ")\n",
    "\n",
    "sample_button.on_click(create_sample)\n",
    "\n",
    "display(widgets.HTML(\"<h3>Create Sample Data</h3>\"))\n",
    "display(sample_button)\n",
    "display(sample_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2379188f-88d6-45f0-b892-fb9934bb7d83",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
